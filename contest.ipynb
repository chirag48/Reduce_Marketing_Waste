{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import necessary modules \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport numpy as np \nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import confusion_matrix, classification_report ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#reading csv for train data\ndata_train=pd.read_csv(\"../input/hackerearths-reduce-marketing-waste/train.csv\")\ndata_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking For Null Values if any Present\ndata_train.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#removing extra special characters from columns for performing operations and coverting object to int or float according to convienence\ndata_train['Deal_value'] = data_train['Deal_value'].str.replace('$', '')\ndata_train['Weighted_amount'] = data_train['Weighted_amount'].str.replace('$', '')\ndata_train['Deal_value']=data_train['Deal_value'].astype('float')\ndata_train['Weighted_amount']=data_train['Weighted_amount'].astype('float')\ndata_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#filling  Numerical null values by mean\ndata_train['Deal_value'] = data_train['Deal_value'].fillna((data_train['Deal_value'].mean()))\ndata_train['Weighted_amount'] = data_train['Weighted_amount'].fillna((data_train['Weighted_amount'].mean()))\ndata_train['Internal_rating'] = data_train['Internal_rating'].fillna((data_train['Internal_rating'].mean()))\ndata_train['Success_probability'] = data_train['Success_probability'].fillna((data_train['Success_probability'].mean()))\ndata_train.head()\n#filling non numerical null values by most occuring one\ndf_most_common_imputed = data_train.apply(lambda x: x.fillna(x.value_counts().index[0]))\ndf_most_common_imputed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#encoding object data type to which it can be fitted to model\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nfor column_name in df_most_common_imputed.columns:\n    if df_most_common_imputed[column_name].dtype == object:\n        df_most_common_imputed[column_name] = le.fit_transform(df_most_common_imputed[column_name])\n    else:\n        pass\n#Selecting best featues according to importance\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\ndf_most_common_imputed[df_most_common_imputed < 0] = 0\nX = df_most_common_imputed.iloc[:,0:]  \ny = df_most_common_imputed.iloc[:,-1] \n\ny=y.astype('int')\nbestfeatures = SelectKBest(score_func=chi2, k=23)\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Features','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(23,'Score')) \ndf_most_common_imputed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#drop other columns other than top 5-6 selected\ndf_most_common_imputed.drop([\"Deal_title\",\"Lead_name\",\"Date_of_creation\",\"Designation\",\"Pitch\",\"Industry\",\"Deal_value\",\"Contact_no\",\"Location\",\"POC_name\",\"Lead_POC_email\",\"Lead_revenue\",\"Resource\",\"Hiring_candidate_role\",\"Last_lead_update\",\"Internal_POC\"], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#BOXPLOT FOR OUTLIAR DETECTION ON TRAINING DATA\na=df_most_common_imputed['Weighted_amount']\nb=df_most_common_imputed['Fund_category']\nc=df_most_common_imputed['Geography']\nd=df_most_common_imputed['Lead_source']\ne=df_most_common_imputed['Level_of_meeting']\nf=df_most_common_imputed['Internal_rating']\nto_plot=[a,b,c,d,e,f]\nfig=plt.figure(1,figsize=(16,9))\nax=fig.add_subplot(111)\nbp=ax.boxplot(to_plot)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" #ScatterPLOT FOR OUTLIAR DETECTION ON TRAINING DATA\nplt.scatter(b,c,e,f,alpha=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ScatterPLOT FOR OUTLIAR DETECTION ON TRAINING DATA\nplt.scatter(a,d,alpha=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#removing Outliars from training data\ndf_most_common_imputed\nimport pandas as pd\nfrom scipy import stats\nz_scores = stats.zscore(df_most_common_imputed)\nabs_z_scores = np.abs(z_scores)\nfiltered_entries = (abs_z_scores < 3).all(axis=1)\nnew_df = df_most_common_imputed[filtered_entries]\nnew_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#reading csv for test data\ndata_test=pd.read_csv(\"../input/hackerearths-reduce-marketing-waste/test.csv\")\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking For Null Values if any Present\ndata_test.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#removing extra special characters from columns for performing operations and coverting object to int or float according to convienence\ndata_test['Deal_value'] = data_test['Deal_value'].str.replace('$', '')\ndata_test['Weighted_amount'] = data_test['Weighted_amount'].str.replace('$', '')\ndata_test['Deal_value']=data_test['Deal_value'].astype('float')\ndata_test['Weighted_amount']=data_test['Weighted_amount'].astype('float')\ndata_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#filling  Numerical null values by mean\ndata_test['Deal_value'] = data_test['Deal_value'].fillna((data_test['Deal_value'].mean()))\ndata_test['Weighted_amount'] = data_test['Weighted_amount'].fillna((data_test['Weighted_amount'].mean()))\ndata_test['Internal_rating'] = data_test['Internal_rating'].fillna((data_test['Internal_rating'].mean()))\ndata_test.head()\n#filling non numerical null values by most occuring one\ndt_most_common_imputed = data_test.apply(lambda x: x.fillna(x.value_counts().index[0]))\ndt_most_common_imputed\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#drop other columns other than top 5-6 selected above\ndt_most_common_imputed.drop([\"Deal_title\",\"Lead_name\",\"Contact_no\",\"Deal_value\",\"Date_of_creation\",\"Designation\",\"Pitch\",\"Industry\",\"Location\",\"POC_name\",\"Lead_POC_email\",\"Lead_revenue\",\"Resource\",\"Hiring_candidate_role\",\"Last_lead_update\",\"Internal_POC\"], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#encoding object data type to which it can be fitted to model\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nfor column_name in dt_most_common_imputed.columns:\n    if dt_most_common_imputed[column_name].dtype == object:\n        dt_most_common_imputed[column_name] = le.fit_transform(dt_most_common_imputed[column_name])\n    else:\n        pass\nfor column_name in df_most_common_imputed.columns:\n    if df_most_common_imputed[column_name].dtype == object:\n        df_most_common_imputed[column_name] = le.fit_transform(df_most_common_imputed[column_name])\n    else:\n        pass\n#splitting data for training and testing\nx_train = new_df.iloc[:,0:-1]  \ny_train = new_df.iloc[:,-1] \nx_test=   dt_most_common_imputed\nx_train\nx_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#BOXPLOT FOR OUTLIAR DETECTION ON TESTING DATA\na=dt_most_common_imputed['Weighted_amount']\nb=dt_most_common_imputed['Fund_category']\nc=dt_most_common_imputed['Geography']\nd=dt_most_common_imputed['Lead_source']\ne=dt_most_common_imputed['Level_of_meeting']\nf=dt_most_common_imputed['Internal_rating']\nto_plot=[a,b,c,d,e,f]\nfig=plt.figure(1,figsize=(16,9))\nax=fig.add_subplot(111)\nbp=ax.boxplot(to_plot)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ScatterPLOT FOR OUTLIAR DETECTION ON TESTING DATA\nplt.scatter(b,c,e,f,alpha=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ScatterPLOT FOR OUTLIAR DETECTION ON TESTING DATA\nplt.scatter(a,d,alpha=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NO OUTLIARS DETECTED NO NEED OF REMOVAL","metadata":{}},{"cell_type":"code","source":"#creating different dataframe for output \ndf=pd.DataFrame()\ndf['Deal_title']=data_test['Deal_title']\ndf['Success_probability']=df_most_common_imputed['Success_probability']\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Applying xgboost as it is too efficient\nimport xgboost as xgb\nxg_reg = xgb.XGBRegressor( learning_rate = 0.101,\n                max_depth =4,objective=\"reg:linear\",alpha =1,n_estimators=9)\nxg_reg.fit(x_train,y_train)\nxgbost= xg_reg.predict(x_test)\ndf[\"Success_probability\"]=xgbost\ndf.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#output CSV\ndf.to_csv(\"xg_boost.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}